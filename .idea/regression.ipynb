{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape \n",
    "\n",
    "        self.w = np.zeros(d)\n",
    "        self.b = 0 \n",
    "\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            y_pred = X @ self.w + self.b \n",
    "\n",
    "            dw = (2 / n) * np.sum(X.T @ (y_pred - y))\n",
    "            db = (2 / n) * np.sum(y_pred - y)\n",
    "\n",
    "            self.w -= self.learning_rate * dw \n",
    "            self.b -= self.learning_rate * db \n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00486102] 0.9857080211211781\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([3, 5, 7, 9])\n",
    "\n",
    "model = LinearRegression(learning_rate=0.01, max_iter=1000)\n",
    "model.fit(X, y)\n",
    "print(model.w, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)  # 初始化权重，形状为 (n_features,)\n",
    "    b = 0  # 初始化偏置\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        y_pred = np.dot(X, w) + b\n",
    "    \n",
    "        dw = (2 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "        db = (2 / n_samples) * np.sum(y_pred - y)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)  \n",
    "    b = 0  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        i = np.random.randint(0, n_samples)\n",
    "        \n",
    "        xi = X[i, :].reshape(1, -1)  \n",
    "        yi = y[i]\n",
    "        \n",
    "        y_pred = np.dot(xi, w) + b\n",
    "        \n",
    "        dw = 2 * np.dot(xi.T, (y_pred - yi))\n",
    "        db = 2 * (y_pred - yi)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, batch_size=2, learning_rate=0.01, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features) \n",
    "    b = 0 \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            \n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "            \n",
    "            dw = (2 / batch_size) * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "            db = (2 / batch_size) * np.sum(y_pred - y_batch)\n",
    "            \n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent: w = [2.00486102], b = 0.9857080211211781\n",
      "Stochastic Gradient Descent: w = [2.00457404], b = [0.98623596]\n",
      "Mini-batch Gradient Descent: w = [2.00022284], b = 0.9993422905558584\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([3, 5, 7, 9])\n",
    "w, b = batch_gradient_descent(X, y)\n",
    "print(f\"Batch Gradient Descent: w = {w}, b = {b}\")\n",
    "\n",
    "w, b = stochastic_gradient_descent(X, y)\n",
    "print(f\"Stochastic Gradient Descent: w = {w}, b = {b}\")\n",
    "\n",
    "w, b = mini_batch_gradient_descent(X, y)\n",
    "print(f\"Mini-batch Gradient Descent: w = {w}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.w = None \n",
    "        self.b = 0 \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape \n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.w -= self.learning_rate * dw \n",
    "            self.b -= self.learning_rate * db \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probabilities = self.predict_proba(X)\n",
    "\n",
    "        return (probabilities >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithRegularization:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, reg_lambda=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs \n",
    "        self.reg_lambda = reg_lambda \n",
    "        self.w = None \n",
    "        self.b = 0 \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape \n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + (self.reg_lambda / n_samples) * self.w \n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.w -= self.learning_rate * dw \n",
    "            self.b -= self.learning_rate * db \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probabilities = self.predict_proba(X)\n",
    "\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [1.32349602], Bias: -3.286820376486958\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([0, 0, 1, 1]) \n",
    "\n",
    "model = LogisticRegressionWithRegularization(learning_rate=0.1, epochs=1000, reg_lambda=0.5)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Weights: {model.w}, Bias: {model.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [2.38668654], Bias: -5.696409201879285\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([0, 0, 1, 1]) \n",
    "\n",
    "model = LogisticRegression(learning_rate=0.1, epochs=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Weights: {model.w}, Bias: {model.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
